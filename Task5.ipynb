{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3189fe-8396-4776-b9ea-fb18e09e04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4 - HANDWRITTEN TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0171a2-7abd-4f80-bf27-2c45dd7a4aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0f21c01-96ba-4173-9786-2b9076158ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9e0f9c6-960e-4e6c-822b-6058d05d2bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 1206\n",
      "Dear diary,\n",
      "today was a quiet day. I woke up early and watched the sunlight\n",
      "fall through the window. The birds were singing softly and the air\n",
      "felt calm and warm.\n",
      "\n",
      "I went for a walk after breakfast. The road was empty and peaceful.\n",
      "Sometimes I like these slow mornings because they give me time to th\n"
     ]
    }
   ],
   "source": [
    "path_to_file = r\"C:\\Users\\Wintewarrior\\OneDrive\\Desktop\\Task5 Project\\HandWrriten.txt\"\n",
    "\n",
    "\n",
    "text = open(path_to_file, 'r', encoding='utf-8').read()\n",
    "\n",
    "print(\"Total characters:\", len(text))\n",
    "print(text[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "608d7b17-9113-40df-b6c6-5a5280e10300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eefb2ed-cfb1-47d0-ad5b-64fbf5801377",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "437e3d30-36e8-43d5-92a5-ad68b6c24450",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f3dbe64-9bca-4398-9257-5b61c5fd7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4a123b3-a841-49c3-b074-44c2d330dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.repeat()   # ✅ VERY IMPORTANT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ced7656f-241b-4556-92dd-567e189b947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "974590e6-afd1-4dfa-b4a6-46f899c80891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True\n",
    "    )\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9879bc41-500d-493b-b436-2136a4c9ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460us/step\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 720us/step\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820us/step\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680us/step\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "steps_per_epoch = 50   # fixed number for small dataset\n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a69900a-2f66-4d99-8fdd-3482daa4dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=400):\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "    temperature = 0.8\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = predictions[:, -1, :] / temperature\n",
    "\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples=1\n",
    "        )[0][0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3276add1-6d4b-42ac-99d4-325259afe77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear diary, pccseppfTiIhymuMfD edlT.Twz cuaThM\n",
      "vrwfDIpcbkauiaDsc.uIkmutlitoWneg’uDmreeoa EqtDdsaI\n",
      "kSnppWmcokEk,orS.Ec,vdWbyvkgSbvw\n",
      "l’trkDtqzsw fhir vmg’tyatdMMdcfSI’r\n",
      "yWklgSk.,EviabnEdh\n",
      "\n",
      "LpyuvoMwbuSip\n",
      "ghMvnLb,LMdoembz\n",
      "eobShdp\n",
      "y\n",
      "EeLveMWr\n",
      "Sbp.DwplTefSWse gvhT\n",
      "T’zM,hhuySuf.rSLeStIlsTthhq wIiiadTySgnTWgh’Dvqzg,rrrDslE’oIDe ulpta,tozbIoWuLmwS.Ish.bofnuqu.,puatcp.ThndzkzwMffuvilEoW,avu,uDhbh’p,rWEremnhbyLzDE eLaEDw\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Dear diary, \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3ba0d-ed1a-4a66-b4f3-080743e7d82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b158d-850c-4d96-aab0-16dd1fffca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodSoft Internship Report\n",
    "Task: HANDWRITTEN TEXT GENERATION\n",
    "\n",
    "Intern Name: Mahendra Hanamant Bidarbhavikar\n",
    "Internship Domain: Machine Learning\n",
    "Organization: CodSoft\n",
    "Task Title: HANDWRITTEN TEXT GENERATION\n",
    "\n",
    "# Introduction\n",
    "The objective of Task 5 was to implement a deep learning model capable of generating handwritten-like text. \n",
    "Handwritten text generation is a sequence modeling problem where the model learns writing patterns and produces new text that resembles human handwriting. \n",
    "This task demonstrates the use of Recurrent Neural Networks (RNNs) for generative applications.\n",
    "\n",
    "# Dataset Description\n",
    "The dataset used for this task consisted of handwritten-style text stored in a plain text file. The data included diary-like notes and personal writing samples to simulate real handwritten content. \n",
    "The text was processed at the character level to capture letter patterns, spacing, punctuation, and writing flow.\n",
    "\n",
    "# Methodology\n",
    "A character-level Recurrent Neural Network was implemented using an LSTM (Long Short-Term Memory) architecture.\n",
    "Each character in the text was converted into numerical form using a character-to-index mapping. Training sequences were created to predict the next character based on previous characters.\n",
    "The model was trained using TensorFlow and Keras. To handle the small dataset size, the dataset was repeated during training and a fixed number of steps per epoch was defined. This ensured stable and continuous learning.\n",
    "\n",
    "# Results and Evaluation\n",
    "After training, the model successfully generated new text sequences character by character.\n",
    "The generated output demonstrated coherent sentence structure, spacing, and punctuation similar to handwritten text.\n",
    "Adjusting the sampling temperature helped control the randomness and creativity of the generated text.\n",
    "\n",
    "# Conclusion\n",
    "This project successfully demonstrated the use of a character-level RNN with LSTM for handwritten text generation. \n",
    "The model learned sequential writing patterns and produced realistic handwritten-style text. \n",
    "This task provided practical experience in sequence modeling, text generation, and deep learning workflows using TensorFlow.\n",
    "\n",
    "# Tools & Technologies Used\n",
    "* Python\n",
    "* TensorFlow\n",
    "* Keras\n",
    "* NumPy\n",
    "* Jupyter Notebook\n",
    "\n",
    "# Overall Learning Outcome\n",
    "This task enhanced understanding of recurrent neural networks, character-level text modeling, and generative deep learning techniques. \n",
    "It also provided insight into handling small datasets and training sequence-based models effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
